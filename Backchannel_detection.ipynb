{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNNXBdI7tZHG",
        "outputId": "9f94ee52-36ac-462d-f783-e7c39d83c6d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer_1 code"
      ],
      "metadata": {
        "id": "33GmCq79aX0v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l-x6I-qFr3Vq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data \n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "import os\n",
        "import csv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "s = nn.Softmax()\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=900):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / (d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # self.mask = torch.cat((torch.zeros(300,1),torch.ones(300,1))).unsqueeze(0)\n",
        "        # self.start = torch.zeros(600,1).unsqueeze(0)\n",
        "        # self.mask = torch.cat((self.start,self.mask),2)\n",
        "        # pe = torch.cat((pe,self.mask),2)\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        # print(self.pe.shape)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # np_arr = self.pe.cpu().detach().numpy()\n",
        "        # np_arr_1 = np.transpose(np_arr, (0, 2, 1))\n",
        "\n",
        "        # np_arr_1 = torch.tensor(np_arr_1)\n",
        "        # x = x + np_arr_1[:, :x.size(2)]\n",
        "        #x = x + self.pe[:, :x.size(2)]]\n",
        "        # print(self.pe[:, :x.size(1)].shape)\n",
        "        # print(x.shape)\n",
        "        #x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "   \n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = s(attn_logits)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "        self.positional_encoding = PositionalEncoding(d_model=block_args[\"input_dim\"])\n",
        "        self.fc = nn.Linear(block_args[\"input_dim\"], 1)\n",
        "    def forward(self, x, mask=None):\n",
        "        X0 = torch.ones(x.shape[0],1,x.shape[2]).to(torch.device('cuda'))\n",
        "        \n",
        "        x = torch.cat((X0,x),1)\n",
        "\n",
        "        x = self.positional_encoding(x)\n",
        "        # print(f'x shape{x.shape}')\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        \n",
        "        return x,torch.sigmoid(self.fc(x))\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        print(len(self.layers))\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            # attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attn_map\n",
        "        # return 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer_2 code"
      ],
      "metadata": {
        "id": "MJhznA_YafcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data \n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "import os\n",
        "import csv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib as plt\n",
        "s = nn.Softmax()\n",
        "class PositionalEncoding_1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=900):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        print(pe.shape)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / (d_model)))\n",
        "        \n",
        "       \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # self.mask = torch.cat((torch.zeros(300,1),torch.ones(300,1))).unsqueeze(0)\n",
        "        # self.start = torch.zeros(600,1).unsqueeze(0)\n",
        "        # self.mask = torch.cat((self.start,self.mask),2)\n",
        "        # pe = torch.cat((pe,self.mask),2)\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        # print(self.pe.shape)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # np_arr = self.pe.cpu().detach().numpy()\n",
        "        # np_arr_1 = np.transpose(np_arr, (0, 2, 1))\n",
        "\n",
        "        # np_arr_1 = torch.tensor(np_arr_1)\n",
        "        # x = x + np_arr_1[:, :x.size(2)]\n",
        "        #x = x + self.pe[:, :x.size(2)]]\n",
        "        # print(self.pe[:, :x.size(1)].shape)\n",
        "        # print(x.shape)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "   \n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = s(attn_logits)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "class MultiheadAttention_1(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "class EncoderBlock_1(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.15):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention_1(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "class TransformerEncoder_1(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock_1(**block_args) for _ in range(num_layers)])\n",
        "        self.positional_encoding = PositionalEncoding_1(d_model=block_args[\"input_dim\"])\n",
        "        self.fc = nn.Linear(block_args[\"input_dim\"], 1)\n",
        "    def forward(self, x, mask=None):\n",
        "        X0 = torch.ones(x.shape[0],1,x.shape[2]).to(torch.device('cuda'))\n",
        "        \n",
        "        x = torch.cat((X0,x),1)\n",
        "\n",
        "        x = self.positional_encoding(x)\n",
        "        # print(f'x shape{x.shape}')\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        \n",
        "        return x,torch.sigmoid(self.fc(x))\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        print(len(self.layers))\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            # attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attn_map\n",
        "        # return 1\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7U0jZmwd7B1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the imports"
      ],
      "metadata": {
        "id": "wJ9kJM2NajcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6imyTDI12HS",
        "outputId": "0e192e97-c79d-4111-af7e-bc38d06fadb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd810dc4930>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from scipy import sparse\n",
        "import sys\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import csv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os.path import exists\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "seed=42\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the train or test data accordingly"
      ],
      "metadata": {
        "id": "fo-wHqE2amZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvjOBewGqa3d",
        "outputId": "4d11f9c6-73d9-40b1-87a2-e6ff3bb5ce7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4898, 90, 750])\n",
            "torch.Size([4898, 1])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "#change the values accordingly to load train data or test data or train data\n",
        "# train_value is set to one to load the train data and also the validation data on which the model is run every epoch.\n",
        "#test_value is set to one to load the test data on the model to evaluate the model\n",
        "\n",
        "train_value = 0\n",
        "test_value = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if train_value == 1:\n",
        "  features = np.load('/content/drive/MyDrive/explicit_npy/features_face_674_90.npy')\n",
        "  labels = np.load('/content/drive/MyDrive/explicit_npy/labels_face_674_90.npy')\n",
        "  features_head = np.load('/content/drive/MyDrive/explicit_npy/features_pose_76_90.npy')\n",
        "  labels_head = np.load('/content/drive/MyDrive/explicit_npy/labels_pose_76_90.npy')\n",
        "  features_tensor = torch.Tensor(features)\n",
        "  # # features_tensor = features_tensor[:,:,3:]\n",
        "  # # to_be_attached_1  = features_tensor[:,:,1:2]\n",
        "  # # # print(features_tensor.shape)\n",
        "  # # # print(to_be_attached_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "  # # features_tensor = features_tensor[:,:,2:]\n",
        "\n",
        "  features_tensor_s = torch.roll(features_tensor, shifts=-1, dims=1)\n",
        "  features_tensor = abs(features_tensor_s-features_tensor)\n",
        "\n",
        "  # features_tensor = torch.cat((features_tensor,to_be_attached_1),2)\n",
        "\n",
        "\n",
        "  features_head = torch.Tensor(features_head)\n",
        "\n",
        "  # # features_head = features_head[:,:,1:]\n",
        "  # # features_head = features_head[:,:,:70]\n",
        "\n",
        "  features_head_s = torch.roll(features_head, shifts=-1, dims=1)\n",
        "  features_head = abs(features_head_s-features_head)\n",
        "  # # print(features_tensor.shape)\n",
        "  # # print(features_head.shape)\n",
        "  features_tensor = torch.cat((features_tensor,features_head),dim = 2)\n",
        "\n",
        "  # # np_arr = features_tensor.numpy()\n",
        "  # # np_arr_1 = np.transpose(np_arr, (0, 2, 1))\n",
        "\n",
        "  # # features_tensor = torch.tensor(np_arr_1)\n",
        "\n",
        "\n",
        "\n",
        "  labels_tensor = torch.Tensor(labels.astype(np.float64))\n",
        "  train_dataset = TensorDataset(features_tensor,labels_tensor)\n",
        "  my_dataloader = torch.utils.data.DataLoader(\n",
        "                  train_dataset,batch_size=batch_size, shuffle=True)\n",
        "  print(features_tensor.shape)\n",
        "  print(labels_tensor.shape)\n",
        "\n",
        "  features_val = np.load('/content/drive/MyDrive/explicit_npy/features_test_face_674_90.npy')\n",
        "  labels_val = np.load('/content/drive/MyDrive/explicit_npy/labels_test_face_674_90.npy')\n",
        "\n",
        "\n",
        "  features_val_head = np.load('/content/drive/MyDrive/explicit_npy/features_test_pose_76_90.npy')\n",
        "  labels_val_head = np.load('/content/drive/MyDrive/explicit_npy/labels_test_pose_76_90.npy')\n",
        "  features_tensor_val = torch.Tensor(features_val)\n",
        "  # features_tensor_test = features_tensor_test[:,:,3:]\n",
        "  # to_be_attached       = features_tensor_test[:,:,1:2]\n",
        "  # print(features_tensor_test.shape)\n",
        "  # print(to_be_attached.shape)\n",
        "\n",
        "\n",
        "\n",
        "  # features_tensor_test = features_tensor_test[:,:,2:]\n",
        "  features_tensor_val_s = torch.roll(features_tensor_val, shifts=-1, dims=1)\n",
        "  features_tensor_val = abs(features_tensor_val_s-features_tensor_val)\n",
        "  # features_tensor_test = torch.cat((features_tensor_test,to_be_attached),2)\n",
        "\n",
        "\n",
        "  features_val_head = torch.Tensor(features_val_head)\n",
        "  # features_test_head = features_test_head[:,:,1:]\n",
        "  # features_test_head = features_test_head[:,:,:70]\n",
        "  features_val_head_s = torch.roll(features_val_head, shifts=-1, dims=1)\n",
        "  features_val_head = abs(features_val_head_s-features_val_head)\n",
        "  features_tensor_val = torch.cat((features_tensor_val,features_val_head),dim = 2)\n",
        "\n",
        "  # features_tensor = torch.cat((features_tensor,features_tensor_test),dim = 0)\n",
        "  # np_arr = features_tensor_test.numpy()\n",
        "  # np_arr_1 = np.transpose(np_arr, (0, 2, 1))\n",
        "\n",
        "  # features_tensor_test = torch.tensor(np_arr_1)\n",
        "\n",
        "\n",
        "  labels_tensor_val = torch.Tensor(labels_val.astype(np.float64))\n",
        "\n",
        "  # labels = torch.cat((labels_tensor,labels_tensor_test),dim = 0)\n",
        "\n",
        "  print(features_tensor_val.shape)\n",
        "  print(labels_tensor_val.shape)\n",
        "\n",
        "  train_dataset_val = TensorDataset(features_tensor_val,labels_tensor_val)\n",
        "  my_dataloader_val = torch.utils.data.DataLoader(\n",
        "                  train_dataset_val,batch_size=batch_size)\n",
        "\n",
        "if test_value ==1 :\n",
        "\n",
        "  features_test = np.load('/content/drive/MyDrive/explicit_npy/features_test_face_674_90_test.npy')\n",
        "  labels_test = np.load('/content/drive/MyDrive/explicit_npy/labels_test_face_674_90.npy')\n",
        "\n",
        "\n",
        "  features_test_head = np.load('/content/drive/MyDrive/explicit_npy/features_test_pose_76_90_test.npy')\n",
        "  labels_test_head = np.load('/content/drive/MyDrive/explicit_npy/labels_test_pose_76_90.npy')\n",
        "  features_tensor_test = torch.Tensor(features_test)\n",
        "  # features_tensor_test = features_tensor_test[:,:,3:]\n",
        "  # to_be_attached       = features_tensor_test[:,:,1:2]\n",
        "  # print(features_tensor_test.shape)\n",
        "  # print(to_be_attached.shape)\n",
        "\n",
        "\n",
        "\n",
        "  # features_tensor_test = features_tensor_test[:,:,2:]\n",
        "  features_tensor_test_s = torch.roll(features_tensor_test, shifts=-1, dims=1)\n",
        "  features_tensor_test = abs(features_tensor_test_s-features_tensor_test)\n",
        "  # features_tensor_test = torch.cat((features_tensor_test,to_be_attached),2)\n",
        "\n",
        "\n",
        "  features_test_head = torch.Tensor(features_test_head)\n",
        "  # features_test_head = features_test_head[:,:,1:]\n",
        "  # features_test_head = features_test_head[:,:,:70]\n",
        "  features_test_head_s = torch.roll(features_test_head, shifts=-1, dims=1)\n",
        "  features_test_head = abs(features_test_head_s-features_test_head)\n",
        "  features_tensor_test = torch.cat((features_tensor_test,features_test_head),dim = 2)\n",
        "\n",
        "  # features_tensor = torch.cat((features_tensor,features_tensor_test),dim = 0)\n",
        "  # np_arr = features_tensor_test.numpy()\n",
        "  # np_arr_1 = np.transpose(np_arr, (0, 2, 1))\n",
        "\n",
        "  # features_tensor_test = torch.tensor(np_arr_1)\n",
        "\n",
        "\n",
        "  #labels_tensor_test = torch.Tensor(labels_test.astype(np.float64))\n",
        "  labels_tensor_test = torch.zeros(4898,1)\n",
        "  # labels = torch.cat((labels_tensor,labels_tensor_test),dim = 0)\n",
        "\n",
        "  print(features_tensor_test.shape)\n",
        "  print(labels_tensor_test.shape)\n",
        "\n",
        "  train_dataset_test = TensorDataset(features_tensor_test,labels_tensor_test)\n",
        "  my_dataloader_test = torch.utils.data.DataLoader(\n",
        "                  train_dataset_test,batch_size=batch_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for training"
      ],
      "metadata": {
        "id": "Ku5nm7MFcgzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pJqz0Fneqa3e"
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rate =0.0005\n",
        "num_epochs = 350\n",
        "device = torch.device('cuda')\n",
        "input_shape  = 750\n",
        "seq_length = 90\n",
        "#learning_rate_decay = 0.99\n",
        "alpha = 0.5\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "B2OZ5gArdnD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5iRS3Vtbqa3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = TransformerEncoder_1(num_layers = 2,input_dim =750,num_heads = 10, dim_feedforward = 1000).to(device)\n",
        "model_2 = TransformerEncoder(num_layers = 1,input_dim =90,num_heads =5, dim_feedforward = 9000).to(device)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,weight_decay = 0.0005)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 0.0005)\n",
        "criterion = nn.BCELoss()\n",
        "ll = count_parameters(model)\n",
        "print(ll)\n",
        "\n",
        "ll2 = count_parameters(model_2)\n",
        "print(ll2)\n",
        "best_val_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    num_samples = 0\n",
        "    model.train()\n",
        "    model_2.train()\n",
        "    for i, (images, labels) in enumerate(my_dataloader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 300, 2048][N,300,128]\n",
        "\n",
        "        \n",
        "        images = images.reshape(-1, seq_length, input_shape ).to(device)\n",
        "      \n",
        "        \n",
        "        # print(images.shape)\n",
        "        labels = labels.to(device)\n",
        "        num_samples+=labels.size(0)\n",
        "        # Forward pass\n",
        "        inpp, outputs = model(images)\n",
        "        inpp = inpp[:,1:,:]\n",
        "        \n",
        "        outputs_1 =outputs[:,0,:]\n",
        "       \n",
        "        \n",
        "        # outputs = outputs.cpu().detach()\n",
        "        outputs = torch.permute(inpp, (0,2,1))\n",
        "        \n",
        "        _,outputs_2 = model_2(outputs)\n",
        "        \n",
        "        \n",
        "        outputs_2 = outputs_2[:,0,:]\n",
        "\n",
        "        # print(outputs_2.shape)\n",
        "        # outputs = torch.cat((outputs,images[0,0,:]),dim = 2)\n",
        "        \n",
        "        # \n",
        "        # print(outputs.shape)\n",
        "        # print(outputs)\n",
        "        # print(f'outputs.shape:{outputs.shape}')\n",
        "        # print(f'outputs.shape:{outputs[:,0,:].shape}')\n",
        "        # print(labels.shape)\n",
        "        # outputs = outputs.squeeze()\n",
        "        \n",
        "        # print(f'labels.shape:{labels.repeat(12).shape}')\n",
        "        loss_1 = criterion(outputs_1, labels.unsqueeze(1))\n",
        "        loss_2 = criterion(outputs_2, labels.unsqueeze(1))\n",
        "\n",
        "        loss = alpha*loss_1 +  (1-alpha)*loss_2\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        predicted = (outputs_2 > 0.5).long()\n",
        "        # print(f'predicted.shape:{predicted.shape}')\n",
        "        # print(f'labels.shape:{labels.shape}')\n",
        "        # print(f'correct_pre:{correct}')\n",
        "        correct += (predicted.squeeze()== labels).sum().item()\n",
        "       \n",
        "    print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
        "          (i , epoch, loss.item(), 100 * correct /num_samples))\n",
        "    # learning_rate *= learning_rate_decay\n",
        "    # update_lr(optimizer, learning_rate)\n",
        "    # writer.add_scalars('Loss',{'train':loss.item()},epoch)\n",
        "    # writer.add_scalars('Accuracy', {'train': 100 * correct /num_samples},epoch)\n",
        "        \n",
        "    # Test the model\n",
        "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "    num_samples_val = 0\n",
        "    model.eval()\n",
        "    model_2.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_val = 0\n",
        "        for i, (images, labels) in enumerate(my_dataloader_val):\n",
        "            #####\n",
        "          images = images.reshape(-1, seq_length, input_shape).to(device)\n",
        "          labels = labels.to(device)\n",
        "          num_samples_val +=labels.size(0)\n",
        "          # Forward pass\n",
        "          inpp, outputs = model(images)\n",
        "          inpp = inpp[:,1:,:]\n",
        "          \n",
        "          outputs_1 =outputs[:,0,:]\n",
        "        \n",
        "          \n",
        "          # outputs = outputs.cpu().detach()\n",
        "          # outputs = torch.permute(inpp, (0,2,1))\n",
        "          # images = torch.permute(images, (0,2,1))\n",
        "          \n",
        "          \n",
        "          outputs = torch.permute(inpp, (0,2,1))\n",
        "        \n",
        "          _,outputs_2 = model_2(outputs)\n",
        "        \n",
        "        \n",
        "          outputs_2 = outputs_2[:,0,:]\n",
        "\n",
        "          # print(outputs_2.shape)\n",
        "          # outputs = torch.cat((outputs,images[0,0,:]),dim = 2)\n",
        "          \n",
        "          # \n",
        "          # print(outputs.shape)\n",
        "          # print(outputs)\n",
        "          # print(f'outputs.shape:{outputs.shape}')\n",
        "          # print(f'outputs.shape:{outputs[:,0,:].shape}')\n",
        "          # print(labels.shape)\n",
        "          # outputs = outputs.squeeze()\n",
        "          \n",
        "          # print(f'labels.shape:{labels.repeat(12).shape}')\n",
        "          loss_1 = criterion(outputs_1, labels.unsqueeze(1))\n",
        "          loss_2 = criterion(outputs_2, labels.unsqueeze(1))\n",
        "\n",
        "\n",
        "          loss = alpha*loss_1 +  (1-alpha)*loss_2\n",
        "          predicted = (outputs_2 > 0.5).long()\n",
        "          correct_val += (predicted.squeeze()== labels).sum().item()\n",
        "        \n",
        "        val_acc = 100 * correct_val / num_samples_val\n",
        "        print(f'Accuracy of the network on the validation: {val_acc} %')\n",
        "        # writer.add_scalars('Accuracy', {'val': val_acc},epoch)\n",
        "    if(val_acc> best_val_acc):\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(),'./best_model'+'.ckpt')\n",
        "        torch.save(model_2.state_dict(),'./best_model_2'+'.ckpt')                           \n",
        "        print(\"best model with val acc \"+ str(best_val_acc)+ \"is saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test loop"
      ],
      "metadata": {
        "id": "QuQ4KTY0d86V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerEncoder_1(num_layers = 2,input_dim =750,num_heads =10, dim_feedforward = 1000).to(device)\n",
        "model_2 = TransformerEncoder(num_layers = 1,input_dim =90,num_heads =5, dim_feedforward = 9000).to(device)\n",
        "model.load_state_dict(torch.load('/content/best_model.ckpt'))\n",
        "model_2.load_state_dict(torch.load('/content/best_model_2.ckpt'))\n",
        "criterion = nn.BCELoss()\n",
        "model.eval()\n",
        "model_2.eval()\n",
        "p1 = []\n",
        "p2 = []\n",
        "num_samples_val = 0\n",
        "df = pd.DataFrame(columns=['feature','labels'])\n",
        "def test():\n",
        "   with torch.no_grad():\n",
        "          correct_val = 0\n",
        "          for i, (images,labels) in enumerate(my_dataloader_test):\n",
        "              #####\n",
        "            images = images.reshape(-1, seq_length, input_shape).to(device)\n",
        "            labels = labels.to(device)\n",
        "            # num_samples_val +=labels.size(0)\n",
        "            # Forward pass\n",
        "            inpp, outputs = model(images)\n",
        "            inpp = inpp[:,1:,:]\n",
        "            \n",
        "            outputs_1 =outputs[:,0,:]\n",
        "          \n",
        "            \n",
        "            # outputs = outputs.cpu().detach()\n",
        "            # outputs = torch.permute(inpp, (0,2,1))\n",
        "            # images = torch.permute(images, (0,2,1))\n",
        "            \n",
        "            \n",
        "            outputs = torch.permute(inpp, (0,2,1))\n",
        "          \n",
        "            outs,outputs_2 = model_2(outputs)\n",
        "            if i==0:\n",
        "             outs_2 = outs.detach().cpu().numpy()\n",
        "             labels_2 = labels.detach().cpu().numpy()\n",
        "\n",
        "            if i !=0:\n",
        "             out_2 =  outs.detach().cpu().numpy()\n",
        "             label_2 = labels.detach().cpu().numpy()\n",
        "             outs_2 = np.append(outs_2,out_2,axis = 0)\n",
        "             labels_2 = np.append(labels_2,label_2)\n",
        "           \n",
        "\n",
        "            \n",
        "          \n",
        "            outputs_2 = outputs_2[:,0,:]\n",
        "\n",
        "\n",
        "\n",
        "            # print(outputs_2.shape)\n",
        "            # outputs = torch.cat((outputs,images[0,0,:]),dim = 2)\n",
        "            \n",
        "            # \n",
        "            # print(outputs.shape)\n",
        "            # print(outputs)\n",
        "            # print(f'outputs.shape:{outputs.shape}')\n",
        "            # print(f'outputs.shape:{outputs[:,0,:].shape}')\n",
        "            # print(labels.shape)\n",
        "            # outputs = outputs.squeeze()\n",
        "            \n",
        "            # print(f'labels.shape:{labels.repeat(12).shape}')\n",
        "            # loss_1 = criterion(outputs_1, labels.unsqueeze(1))\n",
        "            # loss_2 = criterion(outputs_2, labels.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # loss = alpha*loss_1 +  (1-alpha)*loss_2\n",
        "            predicted = (outputs_2 > 0.5).long()\n",
        "\n",
        "            if i==0:\n",
        "             \n",
        "             labels_2_p = predicted.detach().cpu().numpy()\n",
        "\n",
        "            if i !=0:\n",
        "             \n",
        "             label_2_p = predicted.detach().cpu().numpy()\n",
        "             \n",
        "             labels_2_p = np.append(labels_2_p,label_2_p)\n",
        "\n",
        "            # predicted = predicted.detach().cpu()\n",
        "            # predicted = np.array(predicted)\n",
        "            predicted = predicted.tolist()\n",
        "            # print((len(predicted)))\n",
        "            # print(type(predicted[1]))\n",
        "            # p1.append(predicted)\n",
        "            with open('my_file.csv', 'a') as f:\n",
        "             for line in predicted:\n",
        "                f.write(f\"{line}\\n\") \n",
        "\n",
        "        \n",
        "                \n",
        "    "
      ],
      "metadata": {
        "id": "NEQt4xOQdqbz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the below function call generates the csv file with labels."
      ],
      "metadata": {
        "id": "QJABpjYhgxhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCNc2qOBfdV4",
        "outputId": "f26c8aab-0308-4d97-e31c-18d17eb79b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}